{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN / Naive Bayes Based Irony Detector\n",
    "#### Author : Shubhajit Basak\n",
    "\n",
    "##### In this task we will build an Irony Detector or Classifier. It has two part - first we will create a Naive Bayes with Bag Of Words Model. Then in the second part we will create a RNN Sequence Model for the same"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qndnYAQpL6I8"
   },
   "source": [
    "In this task you will develop a system to detect irony in text. We will use the data from the SemEval-2018 task on irony detection. You should use the file `SemEval2018-T3-train-taskA.txt` from Blackboard it consists of examples as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vEvckziyL6I9"
   },
   "source": [
    "```csv\n",
    "Tweet index     Label   Tweet text\n",
    "1       1       Sweet United Nations video. Just in time for Christmas. #imagine #NoReligion  http://t.co/fej2v3OUBR\n",
    "2       1       @mrdahl87 We are rumored to have talked to Erv's agent... and the Angels asked about Ed Escobar... that's hardly nothing    ;)\n",
    "3       1       Hey there! Nice to see you Minnesota/ND Winter Weather \n",
    "4       0       3 episodes left I'm dying over here\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "from random import shuffle\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oc7hUIIjL6I9"
   },
   "source": [
    "# Task 1\n",
    "\n",
    "Read all the data and find the size of vocabulary of the dataset (ignoring case) and the number of positive and negative examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the data from the file\n",
    "inputfile='SemEval2018-T3-train-taskA.txt'\n",
    "f = open(inputfile,'r',encoding=\"utf8\")\n",
    "document = f.readlines()[1:]\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique Vocabulary Count:  13442\n",
      "Positive Examples count:  1911\n",
      "Negetive Example count:  1923\n",
      "Sample Data: \n",
      "\t (1, 1, ['sweet', 'united', 'nations', 'video', '.', 'just', 'in', 'time', 'for', 'christmas', '.', '#', 'imagine', '#', 'noreligion', 'http', ':', '//t.co/fej2v3oubr'])\n"
     ]
    }
   ],
   "source": [
    "positivecounts = 0 # store the Positive Irony Count\n",
    "negetiveCounts = 0 # Store the negetive Irony Count\n",
    "vocab = [] # get a list of all words\n",
    "data =[] # Prepare the data in the intended format of list of tuples\n",
    "# data_Y =[]\n",
    "for line in document:\n",
    "    line_segment = line.split(\"\\t\")\n",
    "    # Check and  increment the positive or negetive class count\n",
    "    if(line_segment[1]==\"1\"):\n",
    "        positivecounts += 1\n",
    "    else:\n",
    "        negetiveCounts += 1\n",
    "    # Tokenise the sentence\n",
    "    sen = word_tokenize(line_segment[2].rstrip().lower())\n",
    "    vocab.append(sen)\n",
    "    # Create the tuple that fits in the required input format\n",
    "    tup = (int(line_segment[0]),int(line_segment[1]),sen)\n",
    "    # Create the data\n",
    "    data.append(tup)\n",
    "# Flatten the Vocab List \n",
    "vocab_flatten = [word.strip() for wordlist in vocab for word in wordlist]\n",
    "# Get the unique Vocab List\n",
    "vocab_flatten_unique = list(set(vocab_flatten))\n",
    "\n",
    "print(\"Unique Vocabulary Count: \", len(vocab_flatten_unique))\n",
    "print(\"Positive Examples count: \", positivecounts)\n",
    "print(\"Negetive Example count: \", negetiveCounts)\n",
    "print(\"Sample Data: \\n\\t\", data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qVdS5s6yL6I-"
   },
   "source": [
    "# Task 2\n",
    "\n",
    "Develop a classifier using the Naive Bayes model to predict if an example is ironic. The model should convert each Tweet into a bag-of-words and calculate\n",
    "\n",
    "$p(\\text{Ironic}|w_1,\\ldots,w_n) \\propto \\prod_{i=1,\\ldots,n} p(w_i \\in \\text{tweet}| \\text{Ironic}) p(\\text{Ironic})$\n",
    "\n",
    "$p(\\text{NotIronic}|w_1,\\ldots,w_n) \\propto \\prod_{i=1,\\ldots,n} p(w_i \\in \\text{tweet}| \\text{NotIronic}) p(\\text{NotIronic})$\n",
    "\n",
    "Use add-alpha smoothing to calculate probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_Naive(train):\n",
    "    # Input is an array of tuple \n",
    "    \n",
    "    wordlist_pos = [] # Array to store word list for positive class\n",
    "    wordlist_neg = [] # Array to store word list for negetive class\n",
    "    positivecounts_train = 0 # Initialise the positive Class Count in Training Dataset\n",
    "    negetiveCounts_train = 0 # Initialise the Negetive Class Count in Test Set\n",
    "    cnt_pos = {} # Dictionary to store word with their respective positive count\n",
    "    cnt_neg = {} # Dictionary to store word with their respective negetive count\n",
    "    for item in train:\n",
    "        #get the positive/negetive data (Bag Of Words) /word count\n",
    "        if(item[1]==1):\n",
    "            positivecounts_train += 1\n",
    "            wordlist_pos.append(item[2])\n",
    "        elif(item[1]==0):\n",
    "            negetiveCounts_train += 1\n",
    "            wordlist_neg.append(item[2]) \n",
    "\n",
    "    # Flatten the word list\n",
    "    wordlist_pos_flatten = [word for item in wordlist_pos for word in item]\n",
    "    wordlist_neg_flatten = [word for item in wordlist_neg for word in item]\n",
    "    \n",
    "    # Populate the dictionary of word count for positive instances\n",
    "    cnt_pos = dict(Counter(wordlist_pos_flatten))\n",
    "\n",
    "    # Populate the dictionary of word count for negetive instances\n",
    "    cnt_neg = dict(Counter(wordlist_neg_flatten))\n",
    "    \n",
    "    # Calculate Prior Probablity for Positive and negetive class in Log Scale\n",
    "    prior_prob_pos = np.log(positivecounts_train/(positivecounts_train + negetiveCounts_train))\n",
    "    prior_prob_neg = np.log(negetiveCounts_train/(positivecounts_train + negetiveCounts_train))\n",
    "    \n",
    "    # get the unique vocab count in the traing set\n",
    "    vocab_train = list(set(wordlist_pos_flatten)) + list(set(wordlist_neg_flatten))\n",
    "    vocab_count = len(vocab_train)\n",
    "    \n",
    "    # Return Prior Probablities, positive, negetive word count dictionary, total unique vocab count\n",
    "    param = (prior_prob_pos,prior_prob_neg,cnt_pos,cnt_neg,vocab_count)\n",
    "    return param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_Naive(text,param):\n",
    "    # Input Sample Text and the Traing Model Input\n",
    "    \n",
    "    (prior_prob_pos,prior_prob_neg,cnt_pos,cnt_neg,vocab_count) = param\n",
    "    # Initialise the Probablity for the Positive and Negetive Class\n",
    "    prob_pos = prior_prob_pos\n",
    "    prob_neg = prior_prob_neg\n",
    "    \n",
    "    # Calculate the Conditional Probablity with Add One Smoothing \n",
    "    for word in text:\n",
    "        if(word in cnt_pos.keys()):\n",
    "            prob_cond_pos = np.log((cnt_pos[word]+1)/(sum(cnt_pos.values()) + vocab_count))\n",
    "            prob_pos += prob_cond_pos\n",
    "        else:\n",
    "            prob_cond_pos = np.log((1)/(sum(cnt_pos.values()) + vocab_count))\n",
    "            prob_pos += prob_cond_pos\n",
    "        \n",
    "        if(word in cnt_neg.keys()):\n",
    "            prob_cond_neg = np.log((cnt_neg[word]+1)/(sum(cnt_neg.values()) + vocab_count))\n",
    "            prob_neg += prob_cond_neg\n",
    "        else:\n",
    "            prob_cond_neg = np.log((1)/(sum(cnt_neg.values()) + vocab_count))\n",
    "            prob_neg += prob_cond_neg\n",
    "    \n",
    "    # Check for Highest Probablity Class and Return\n",
    "    if(prob_pos > prob_neg):\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "w5LivWxOL6JA"
   },
   "source": [
    "# Task 3 \n",
    "\n",
    "Divide the data into a training and test set and justify your split.\n",
    "\n",
    "Choose a suitable evaluation metric and implement it. Explain why you chose this evaluation metric.\n",
    "\n",
    "Evaluate the method in Task 2 according to this metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method to split the train test data\n",
    "def train_test_split(data, test_size=0.33):\n",
    "    shuffle(data) # Shuffle the data randomly\n",
    "    limit = round(test_size*len(data))\n",
    "    # Split the data\n",
    "    test = data[0:limit]\n",
    "    train = data[limit:]\n",
    "    return (train,test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method to calculate Accuracy, Precision, Recall & fscore\n",
    "def evaluation(actual,prediction):\n",
    "    # Input array of actual and prediction in Counter Format\n",
    "    counts = Counter(zip(prediction, actual))\n",
    "    \n",
    "    # Calculate the TP,TN,FP,FN Counts\n",
    "    true_pos  = counts[1, 1]\n",
    "    true_neg  = counts[0, 0]\n",
    "    false_pos = counts[1, 0]\n",
    "    false_neg = counts[0, 1]\n",
    "    \n",
    "    # Calculate Accuracy, Precision, Recall & fscore\n",
    "    accuracy = (true_pos + true_neg) / float(len(actual)) if actual else 0\n",
    "    recall = true_pos / float(true_pos + false_neg) if (true_pos + false_neg) else 0\n",
    "    precision = true_pos / float(true_pos + false_pos) if (true_pos + false_neg) else 0\n",
    "    fscore = 2*precision*recall / (precision + recall) if (precision + recall) else 0\n",
    "    \n",
    "    # Print and Return\n",
    "    print(\"Accuracy : \", accuracy)\n",
    "    print(\"Recall : \", recall)\n",
    "    print(\"Precision : \", precision)\n",
    "    print(\"FScore : \",fscore)\n",
    "    return accuracy, precision, recall, fscore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split The data \n",
    "train,test = train_test_split(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation for Naive Bayes:\n",
      "\n",
      "Accuracy :  0.6592885375494071\n",
      "Recall :  0.6977491961414791\n",
      "Precision :  0.6410635155096012\n",
      "FScore :  0.668206312548114\n",
      "(0.6592885375494071, 0.6410635155096012, 0.6977491961414791, 0.668206312548114)\n"
     ]
    }
   ],
   "source": [
    "# Train the data\n",
    "param = train_Naive(train)\n",
    "\n",
    "# Predict with the Model parameters\n",
    "prediction_naive = [predict_Naive(item[2],param) for item in test]\n",
    "\n",
    "# Actual labels\n",
    "actual = [item[1] for item in test]\n",
    "\n",
    "# Get Evaluation Scores with the actual labels\n",
    "print(\"Evaluation for Naive Bayes:\\n\")\n",
    "print(evaluation(actual,prediction_naive))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "82JnhmgBL6JA"
   },
   "source": [
    "# Task 4\n",
    "\n",
    "Run the following code to generate a model from your training set. The training set should be in a variable  called `train` and is assumed to be of the form:\n",
    "\n",
    "```\n",
    "[(1, 1, ['sweet', 'united', 'nations', 'video', '.', 'just', 'in', 'time', 'for', 'christmas', '.', '#', 'imagine', '#', 'noreligion', 'http', ':', '//t.co/fej2v3oubr']), \n",
    " (2, 1, ['@', 'mrdahl87', 'we', 'are', 'rumored', 'to', 'have', 'talked', 'to', 'erv', \"'s\", 'agent', '...', 'and', 'the', 'angels', 'asked', 'about', 'ed', 'escobar', '...', 'that', \"'s\", 'hardly', 'nothing', ';', ')']), \n",
    " (3, 1, ['hey', 'there', '!', 'nice', 'to', 'see', 'you', 'minnesota/nd', 'winter', 'weather']), \n",
    " (4, 0, ['3', 'episodes', 'left', 'i', \"'m\", 'dying', 'over', 'here']), \n",
    " ...\n",
    "]\n",
    " ```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(270, 1, ['@', 'patevans', '@', 'grbj', 'too', 'sad', '...', 'local', 'now', '!']), (3350, 0, ['there', 'was', 'a', 'shooting', 'in', 'my', 'old', 'neighborhood', 'where', 'my', 'family', 'lives', '.', 'praying', 'everyone', 'is', 'okay', 'and', 'stays', 'safe', ':', 'heavy_black_heart', ':', '️'])]\n"
     ]
    }
   ],
   "source": [
    "# Print an element\n",
    "print(train[0:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\envs\\ratenv\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "25/25 [==============================] - 4s 157ms/step - loss: 0.6929 - categorical_accuracy: 0.5140 - val_loss: 0.6970 - val_categorical_accuracy: 0.3958\n",
      "Epoch 2/50\n",
      "25/25 [==============================] - 1s 38ms/step - loss: 0.6900 - categorical_accuracy: 0.5512 - val_loss: 0.6949 - val_categorical_accuracy: 0.4550\n",
      "Epoch 3/50\n",
      "25/25 [==============================] - 1s 37ms/step - loss: 0.6966 - categorical_accuracy: 0.4568 - val_loss: 0.6917 - val_categorical_accuracy: 0.5033\n",
      "Epoch 4/50\n",
      "25/25 [==============================] - 1s 37ms/step - loss: 0.6847 - categorical_accuracy: 0.5800 - val_loss: 0.6839 - val_categorical_accuracy: 0.5592\n",
      "Epoch 5/50\n",
      "25/25 [==============================] - 1s 37ms/step - loss: 0.6789 - categorical_accuracy: 0.5608 - val_loss: 0.6808 - val_categorical_accuracy: 0.5542\n",
      "Epoch 6/50\n",
      "25/25 [==============================] - 1s 37ms/step - loss: 0.6846 - categorical_accuracy: 0.5868 - val_loss: 0.6669 - val_categorical_accuracy: 0.6208\n",
      "Epoch 7/50\n",
      "25/25 [==============================] - 1s 37ms/step - loss: 0.6499 - categorical_accuracy: 0.6644 - val_loss: 0.6773 - val_categorical_accuracy: 0.5617\n",
      "Epoch 8/50\n",
      "25/25 [==============================] - 1s 37ms/step - loss: 0.6424 - categorical_accuracy: 0.6456 - val_loss: 0.6753 - val_categorical_accuracy: 0.5642\n",
      "Epoch 9/50\n",
      "25/25 [==============================] - 1s 37ms/step - loss: 0.6340 - categorical_accuracy: 0.6800 - val_loss: 0.7157 - val_categorical_accuracy: 0.5108\n",
      "Epoch 10/50\n",
      "25/25 [==============================] - 1s 37ms/step - loss: 0.6033 - categorical_accuracy: 0.7028 - val_loss: 0.7414 - val_categorical_accuracy: 0.5425\n",
      "Epoch 11/50\n",
      "25/25 [==============================] - 1s 37ms/step - loss: 0.6081 - categorical_accuracy: 0.6876 - val_loss: 0.6870 - val_categorical_accuracy: 0.5742\n",
      "Epoch 12/50\n",
      "25/25 [==============================] - 1s 37ms/step - loss: 0.5692 - categorical_accuracy: 0.7256 - val_loss: 0.7219 - val_categorical_accuracy: 0.5808\n",
      "Epoch 13/50\n",
      "25/25 [==============================] - 1s 37ms/step - loss: 0.4896 - categorical_accuracy: 0.7884 - val_loss: 0.7772 - val_categorical_accuracy: 0.5467\n",
      "Epoch 14/50\n",
      "25/25 [==============================] - 1s 37ms/step - loss: 0.4748 - categorical_accuracy: 0.7900 - val_loss: 0.8049 - val_categorical_accuracy: 0.5558\n",
      "Epoch 15/50\n",
      "25/25 [==============================] - 1s 37ms/step - loss: 0.4442 - categorical_accuracy: 0.7988 - val_loss: 0.9598 - val_categorical_accuracy: 0.5083\n",
      "Epoch 16/50\n",
      "25/25 [==============================] - 1s 37ms/step - loss: 0.4266 - categorical_accuracy: 0.7976 - val_loss: 0.8629 - val_categorical_accuracy: 0.5600\n",
      "Epoch 17/50\n",
      "25/25 [==============================] - 1s 37ms/step - loss: 0.4659 - categorical_accuracy: 0.7784 - val_loss: 0.8282 - val_categorical_accuracy: 0.5267\n",
      "Epoch 18/50\n",
      "25/25 [==============================] - 1s 37ms/step - loss: 0.4061 - categorical_accuracy: 0.8092 - val_loss: 0.8796 - val_categorical_accuracy: 0.5850\n",
      "Epoch 19/50\n",
      "25/25 [==============================] - 1s 37ms/step - loss: 0.3564 - categorical_accuracy: 0.8284 - val_loss: 0.8947 - val_categorical_accuracy: 0.5600\n",
      "Epoch 20/50\n",
      "25/25 [==============================] - 1s 36ms/step - loss: 0.3800 - categorical_accuracy: 0.8132 - val_loss: 0.9143 - val_categorical_accuracy: 0.5558\n",
      "Epoch 21/50\n",
      "25/25 [==============================] - 1s 37ms/step - loss: 0.3395 - categorical_accuracy: 0.8380 - val_loss: 1.0398 - val_categorical_accuracy: 0.5083\n",
      "Epoch 22/50\n",
      "25/25 [==============================] - 1s 37ms/step - loss: 0.3360 - categorical_accuracy: 0.8392 - val_loss: 1.0342 - val_categorical_accuracy: 0.5375\n",
      "Epoch 23/50\n",
      "25/25 [==============================] - 1s 37ms/step - loss: 0.3730 - categorical_accuracy: 0.8128 - val_loss: 0.8987 - val_categorical_accuracy: 0.5442\n",
      "Epoch 24/50\n",
      "25/25 [==============================] - 1s 37ms/step - loss: 0.3235 - categorical_accuracy: 0.8496 - val_loss: 1.0387 - val_categorical_accuracy: 0.5658\n",
      "Epoch 25/50\n",
      "25/25 [==============================] - 1s 37ms/step - loss: 0.3132 - categorical_accuracy: 0.8488 - val_loss: 0.9847 - val_categorical_accuracy: 0.5558\n",
      "Epoch 26/50\n",
      "25/25 [==============================] - 1s 37ms/step - loss: 0.3291 - categorical_accuracy: 0.8380 - val_loss: 0.9492 - val_categorical_accuracy: 0.5783\n",
      "Epoch 27/50\n",
      "25/25 [==============================] - 1s 37ms/step - loss: 0.2979 - categorical_accuracy: 0.8516 - val_loss: 1.0631 - val_categorical_accuracy: 0.5000\n",
      "Epoch 28/50\n",
      "25/25 [==============================] - 1s 37ms/step - loss: 0.3280 - categorical_accuracy: 0.8424 - val_loss: 1.0575 - val_categorical_accuracy: 0.5442\n",
      "Epoch 29/50\n",
      "25/25 [==============================] - 1s 36ms/step - loss: 0.3386 - categorical_accuracy: 0.8280 - val_loss: 0.9780 - val_categorical_accuracy: 0.5325\n",
      "Epoch 30/50\n",
      "25/25 [==============================] - 1s 37ms/step - loss: 0.2913 - categorical_accuracy: 0.8548 - val_loss: 1.0569 - val_categorical_accuracy: 0.5833\n",
      "Epoch 31/50\n",
      "25/25 [==============================] - 1s 37ms/step - loss: 0.2870 - categorical_accuracy: 0.8616 - val_loss: 1.0910 - val_categorical_accuracy: 0.5392\n",
      "Epoch 32/50\n",
      "25/25 [==============================] - 1s 37ms/step - loss: 0.3062 - categorical_accuracy: 0.8404 - val_loss: 0.9892 - val_categorical_accuracy: 0.5808\n",
      "Epoch 33/50\n",
      "25/25 [==============================] - 1s 37ms/step - loss: 0.3024 - categorical_accuracy: 0.8492 - val_loss: 1.1041 - val_categorical_accuracy: 0.5067\n",
      "Epoch 34/50\n",
      "25/25 [==============================] - 1s 37ms/step - loss: 0.3109 - categorical_accuracy: 0.8444 - val_loss: 1.1463 - val_categorical_accuracy: 0.5200\n",
      "Epoch 35/50\n",
      "25/25 [==============================] - 1s 37ms/step - loss: 0.3084 - categorical_accuracy: 0.8392 - val_loss: 1.0007 - val_categorical_accuracy: 0.5433\n",
      "Epoch 36/50\n",
      "25/25 [==============================] - 1s 37ms/step - loss: 0.2752 - categorical_accuracy: 0.8656 - val_loss: 1.0944 - val_categorical_accuracy: 0.5725\n",
      "Epoch 37/50\n",
      "25/25 [==============================] - 1s 37ms/step - loss: 0.2717 - categorical_accuracy: 0.8640 - val_loss: 1.1047 - val_categorical_accuracy: 0.5292\n",
      "Epoch 38/50\n",
      "25/25 [==============================] - 1s 37ms/step - loss: 0.2983 - categorical_accuracy: 0.8460 - val_loss: 1.0776 - val_categorical_accuracy: 0.5525\n",
      "Epoch 39/50\n",
      "25/25 [==============================] - 1s 36ms/step - loss: 0.2721 - categorical_accuracy: 0.8656 - val_loss: 1.2261 - val_categorical_accuracy: 0.5042\n",
      "Epoch 40/50\n",
      "25/25 [==============================] - 1s 37ms/step - loss: 0.3101 - categorical_accuracy: 0.8392 - val_loss: 1.1520 - val_categorical_accuracy: 0.5183\n",
      "Epoch 41/50\n",
      "25/25 [==============================] - 1s 37ms/step - loss: 0.2912 - categorical_accuracy: 0.8416 - val_loss: 1.0842 - val_categorical_accuracy: 0.5500\n",
      "Epoch 42/50\n",
      "25/25 [==============================] - 1s 37ms/step - loss: 0.2614 - categorical_accuracy: 0.8640 - val_loss: 1.0991 - val_categorical_accuracy: 0.5667\n",
      "Epoch 43/50\n",
      "25/25 [==============================] - 1s 37ms/step - loss: 0.2697 - categorical_accuracy: 0.8572 - val_loss: 1.1624 - val_categorical_accuracy: 0.5283\n",
      "Epoch 44/50\n",
      "25/25 [==============================] - 1s 37ms/step - loss: 0.2991 - categorical_accuracy: 0.8456 - val_loss: 1.0872 - val_categorical_accuracy: 0.5733\n",
      "Epoch 45/50\n",
      "25/25 [==============================] - 1s 37ms/step - loss: 0.2596 - categorical_accuracy: 0.8680 - val_loss: 1.2790 - val_categorical_accuracy: 0.5142\n",
      "Epoch 46/50\n",
      "25/25 [==============================] - 1s 37ms/step - loss: 0.2902 - categorical_accuracy: 0.8444 - val_loss: 1.2408 - val_categorical_accuracy: 0.5300\n",
      "Epoch 47/50\n",
      "25/25 [==============================] - 1s 36ms/step - loss: 0.2746 - categorical_accuracy: 0.8608 - val_loss: 1.1724 - val_categorical_accuracy: 0.5217\n",
      "Epoch 48/50\n",
      "25/25 [==============================] - 1s 37ms/step - loss: 0.2649 - categorical_accuracy: 0.8684 - val_loss: 1.0400 - val_categorical_accuracy: 0.5842\n",
      "Epoch 49/50\n",
      "25/25 [==============================] - 1s 37ms/step - loss: 0.2656 - categorical_accuracy: 0.8616 - val_loss: 1.2679 - val_categorical_accuracy: 0.5217\n",
      "Epoch 50/50\n",
      "25/25 [==============================] - 1s 37ms/step - loss: 0.2859 - categorical_accuracy: 0.8508 - val_loss: 1.1628 - val_categorical_accuracy: 0.5608\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense, Activation, Embedding, Dropout, TimeDistributed\n",
    "from keras.layers import LSTM\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils import to_categorical\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "import numpy as np\n",
    "\n",
    "## These values should be set from Task 3\n",
    "train, test = train,test # Take the same test and trian from yhe previous task\n",
    "\n",
    "# Make dictionary from the test and train data\n",
    "def make_dictionary(train, test):\n",
    "    dictionary = {}\n",
    "    for d in train+test:\n",
    "        for w in d[2]:\n",
    "            if w not in dictionary:\n",
    "                dictionary[w] = len(dictionary)\n",
    "    return dictionary\n",
    "\n",
    "class KerasBatchGenerator(object):\n",
    "    def __init__(self, data, num_steps, batch_size, vocabulary, skip_step=5):\n",
    "        self.data = data\n",
    "        self.num_steps = num_steps\n",
    "        self.batch_size = batch_size\n",
    "        self.vocabulary = vocabulary\n",
    "        self.current_idx = 0\n",
    "        self.current_sent = 0\n",
    "        self.skip_step = skip_step\n",
    "\n",
    "    def generate(self):\n",
    "        x = np.zeros((self.batch_size, self.num_steps))\n",
    "        y = np.zeros((self.batch_size, self.num_steps, 2))\n",
    "        while True:\n",
    "            for i in range(self.batch_size):\n",
    "                # Choose a sentence and position with at lest num_steps more words\n",
    "                while self.current_idx + self.num_steps >= len(self.data[self.current_sent][2]):\n",
    "                    self.current_idx = self.current_idx % len(self.data[self.current_sent][2])\n",
    "                    self.current_sent += 1\n",
    "                    if self.current_sent >= len(self.data):\n",
    "                        self.current_sent = 0\n",
    "                # The rows of x are set to values like [1,2,3,4,5]\n",
    "                x[i, :] = [self.vocabulary[w] for w in self.data[self.current_sent][2][self.current_idx:self.current_idx + self.num_steps]]\n",
    "                # The rows of y are set to values like [[1,0],[1,0],[1,0],[1,0],[1,0]]\n",
    "                y[i, :, :] = [[self.data[self.current_sent][1], 1-self.data[self.current_sent][1]]] * self.num_steps\n",
    "                self.current_idx += self.skip_step\n",
    "            yield x, y\n",
    "\n",
    "# Hyperparameters for model\n",
    "vocabulary = make_dictionary(train, test)\n",
    "num_steps = 5\n",
    "batch_size = 20\n",
    "num_epochs = 50 # Reduce this if the model is taking too long to train (or increase for performance)\n",
    "hidden_size = 50 # Increase this to improve perfomance (or increase for performance)\n",
    "use_dropout=True\n",
    "\n",
    "# Create batches for RNN\n",
    "train_data_generator = KerasBatchGenerator(train, num_steps, batch_size, vocabulary,\n",
    "                                           skip_step=num_steps)\n",
    "valid_data_generator = KerasBatchGenerator(test, num_steps, batch_size, vocabulary,\n",
    "                                           skip_step=num_steps)\n",
    "\n",
    "# A double stacked LSTM with dropout and n hidden layers\n",
    "model = Sequential()\n",
    "model.add(Embedding(len(vocabulary), hidden_size, input_length=num_steps))\n",
    "model.add(LSTM(hidden_size, return_sequences=True))\n",
    "model.add(LSTM(hidden_size, return_sequences=True))\n",
    "if use_dropout:\n",
    "    model.add(Dropout(0.5))\n",
    "model.add(TimeDistributed(Dense(2)))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "# Set optimizer and build model\n",
    "optimizer = Adam()\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['categorical_accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit_generator(train_data_generator.generate(), len(train)//(batch_size*num_steps), num_epochs,\n",
    "                        validation_data=valid_data_generator.generate(),\n",
    "                        validation_steps=len(test)//(batch_size*num_steps))\n",
    "\n",
    "# Save the model\n",
    "model.save(\"final_model.hdf5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qLYwZTAVL6JH"
   },
   "source": [
    "Now consider the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0.358755   0.641245  ]\n",
      "  [0.3049212  0.69507873]\n",
      "  [0.42448446 0.5755155 ]\n",
      "  [0.02209741 0.9779026 ]\n",
      "  [0.57697505 0.42302498]]]\n"
     ]
    }
   ],
   "source": [
    "model = load_model(\"final_model.hdf5\")\n",
    "\n",
    "x = np.zeros((1,num_steps))\n",
    "x[0,:] = [vocabulary[\"this\"],vocabulary[\"the\"],vocabulary[\"an\"],vocabulary[\"easy\"],vocabulary[\"test\"]]\n",
    "print(model.predict(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zh8vY22gL6JL"
   },
   "source": [
    "Using the code above write a function that can predict the label using the LSTM model above and compare it with the evaluation performed in Task 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = load_model(\"final_model.hdf5\")\n",
    "def predict_LSTM(num_steps,test):\n",
    "    prediction_LSTM = []\n",
    "    print(\"Total Item: \",len(test))\n",
    "    # Loop Through all the Records in the Test Case\n",
    "    for i1 in range(len(test)):\n",
    "        # Extract the sentence from the data\n",
    "        line = test[i1][2]\n",
    "        dictPos = defaultdict(list)\n",
    "        dictNeg = defaultdict(list)\n",
    "        \n",
    "        # Loop Through the Element in windows of stepsize\n",
    "        for i in range(0,(len(line)-(num_steps-1))):\n",
    "            # take the words of step size\n",
    "            str_ln = line[i:i+num_steps]\n",
    "            # convert words to corresponding vocab index\n",
    "            lin_seg = np.array([vocabulary[i] for i in line[i:i+num_steps]])\n",
    "            # reshape the array as (1,number of steps)\n",
    "            lin_seg = lin_seg.reshape(1,num_steps)\n",
    "            # predict the line with the model\n",
    "            lin_seg_pred = model.predict(lin_seg)\n",
    "            # take the first column of the probablity output matrix to get the positive class probablity\n",
    "            lin_seg_pos = lin_seg_pred.reshape(num_steps,2)[:,0:1].flatten()\n",
    "            # take the second column of the probablity output matrix to get the negetive class probablity\n",
    "            lin_seg_neg = lin_seg_pred.reshape(num_steps,2)[:,1:].flatten()\n",
    "            \n",
    "            # Create a dictionary for (word,Positive Probablity)\n",
    "            d_pos = dict(zip(str_ln,lin_seg_pos))\n",
    "            # Convert the value of dictionary into a list\n",
    "            d_pos = {k: [v] for k, v in d_pos.items()}\n",
    "            # Create a dictionary for (word,Negetive Probablity)\n",
    "            d_neg = dict(zip(str_ln,lin_seg_neg))\n",
    "            # Convert the value of dictionary into a list\n",
    "            d_neg = {k: [v] for k, v in d_neg.items()}\n",
    "\n",
    "            # Merge the Dictionary \n",
    "            for k,v in d_pos.items():\n",
    "                dictPos[k].extend(v)\n",
    "\n",
    "            for k,v in d_neg.items():\n",
    "                dictNeg[k].extend(v)\n",
    "\n",
    "        # Koustava (#18234857) : Code Start\n",
    "        multiplyPositive=1\n",
    "        multiplyNegative=1\n",
    "        # Multiply all the Value elements of the list to get the maximum positive probablity\n",
    "        for k,v in dictPos.items():\n",
    "            for j1 in v:\n",
    "                multiplyPositive = multiplyPositive * j1\n",
    "        # Multiply all the Value elements of the list to get the maximum negetive probablity\n",
    "        for k1,v1 in dictNeg.items():\n",
    "            for j2 in v1:\n",
    "                multiplyNegative = multiplyNegative * j2\n",
    "        \n",
    "        # Check for maximum probablity and assign the class\n",
    "        if(multiplyPositive > multiplyNegative):\n",
    "            prediction_LSTM.append(1)\n",
    "        else:\n",
    "            prediction_LSTM.append(0)\n",
    "        # Koustava (#18234857) : Code End\n",
    "        \n",
    "        # Print 100th iteration to check the progress\n",
    "        if(i1 % 100 == 0):\n",
    "            print(\"Iteration complete: {0}\".format(i1))\n",
    "    \n",
    "    # Return the Predition list of class\n",
    "    return prediction_LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please take a note that the following code will take time to run as it is iterating each element to calculate the accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Item:  1265\n",
      "Iteration complete: 0\n",
      "Iteration complete: 100\n",
      "Iteration complete: 200\n",
      "Iteration complete: 300\n",
      "Iteration complete: 400\n",
      "Iteration complete: 500\n",
      "Iteration complete: 600\n",
      "Iteration complete: 700\n",
      "Iteration complete: 800\n",
      "Iteration complete: 900\n",
      "Iteration complete: 1000\n",
      "Iteration complete: 1100\n",
      "Iteration complete: 1200\n"
     ]
    }
   ],
   "source": [
    "# Predict with the LSTM Model\n",
    "prediction_LSTM = predict_LSTM(num_steps,test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy :  0.5762845849802372\n",
      "Recall :  0.5594855305466238\n",
      "Precision :  0.5704918032786885\n",
      "FScore :  0.564935064935065\n",
      "(0.5762845849802372, 0.5704918032786885, 0.5594855305466238, 0.564935064935065)\n"
     ]
    }
   ],
   "source": [
    "# Print the evaluation Matrix\n",
    "print(evaluation(actual,prediction_LSTM))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9yZucGZmL6JM"
   },
   "source": [
    "# Task 5\n",
    "\n",
    "An improvement to either the system developed in Task 2 or 4 and show that it improves according to your evaluation metric.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Approach :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above approach we have seen the LSTM Based Model is giving as accuracy of around 58%. \n",
    "We have found the following observations on the approach :\n",
    "\n",
    "* The approach is ignoring the sentences which is of length less than the step size\n",
    "* It also ignores the part of the sentences which has length greater than the step size\n",
    "* As we are using the NLTK Tokenizer it will split all the punctuations, numbers and symbols as well which might affect the performance \n",
    "\n",
    "So I have implemented the following improvements in the system - \n",
    "\n",
    "* I have decided not to use the NLTK tokenizer but to remove the punctuations and other extra symbols and http links manually from the data, this will prevent tokenizing those numbers and symbols further\n",
    "* After removing the ymbols I am using Keras Preprocessing Tokenizer and convert them into sequence of numbers\n",
    "* Convert each sentence with the numeric vector with the same length\n",
    "* To get the same length I am using padding to pad extra items in my vector by a default value\n",
    "* I have not changed anything significantly in the model\n",
    "* I have updated the hyperparameters as now my input feature vector dimension has increased significantly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# from sklearn.feature_extraction.text import CountVectorizer\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "# from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, LSTM, SpatialDropout1D\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "# from keras.models import Sequential, load_model\n",
    "from keras.layers import Activation, Dropout, TimeDistributed,Flatten\n",
    "from keras.regularizers import l1\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils import to_categorical\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Data: \n",
      "\t [1, 'Sweet United Nations video. Just in time for Christmas. #imagine #NoReligion  http://t.co/fej2v3OUBR']\n"
     ]
    }
   ],
   "source": [
    "data =[] # Prepare the data in the intended format of list of tuples\n",
    "# data_Y =[]\n",
    "for line in document:\n",
    "    line_segment = line.split(\"\\t\")\n",
    "    # Create a list of labels and sentence\n",
    "    item = [int(line_segment[1]),line_segment[2].strip()]\n",
    "    # Create the data\n",
    "    data.append(item)\n",
    "\n",
    "print(\"Sample Data: \\n\\t\", data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Irony</th>\n",
       "      <th>Sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Sweet United Nations video. Just in time for C...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>@mrdahl87 We are rumored to have talked to Erv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Hey there! Nice to see you Minnesota/ND Winter...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>3 episodes left I'm dying over here</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>\"I can't breathe!\" was chosen as the most nota...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Irony                                           Sentence\n",
       "0      1  Sweet United Nations video. Just in time for C...\n",
       "1      1  @mrdahl87 We are rumored to have talked to Erv...\n",
       "2      1  Hey there! Nice to see you Minnesota/ND Winter...\n",
       "3      0                3 episodes left I'm dying over here\n",
       "4      1  \"I can't breathe!\" was chosen as the most nota..."
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert into a dataframe \n",
    "data_df = pd.DataFrame(data)\n",
    "data_df.columns = [\"Irony\",\"Sentence\"]\n",
    "data_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the labels into numbers\n",
    "data_df['Irony'] = pd.to_numeric(data_df['Irony'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df['Sentence'] = data_df['Sentence'].apply(lambda x: x.lower()) # Make Lower\n",
    "data_df['Sentence'] = data_df['Sentence'].apply((lambda x: re.sub(r'https?://\\S+','',x))) # Remove the http links\n",
    "data_df['Sentence'] = data_df['Sentence'].apply((lambda x: re.sub('[^a-zA-z0-9#\\s]','',x))) # Remove the symbols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sweet united nations video just in time for christmas #imagine #noreligion  \n"
     ]
    }
   ],
   "source": [
    "# Print sample data\n",
    "print(data_df.iloc[0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13191"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def make_dictionary(data):\n",
    "    dictionary = {}\n",
    "    for i in range(0,len(data)):\n",
    "        for w in data.iloc[i,1].split(' '):\n",
    "            if w not in dictionary:\n",
    "                dictionary[w] = len(dictionary)+1\n",
    "    return dictionary\n",
    "\n",
    "# get the dictionary\n",
    "dic = make_dictionary(data_df)\n",
    "len(dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Take the max feature length from the dictionary\n",
    "max_fatures = len(dic)\n",
    "# Tokenize with Keras Tokenizer with a space \n",
    "tokenizer = Tokenizer(num_words=max_fatures, split=' ')\n",
    "# Convert Text to Numeric values\n",
    "tokenizer.fit_on_texts(data_df['Sentence'].values)\n",
    "X = tokenizer.texts_to_sequences(data_df['Sentence'].values)\n",
    "# Pad zeros to get the vectors of same length\n",
    "X = pad_sequences(X, padding='post')\n",
    "# get the labels\n",
    "Y = pd.get_dummies(data_df['Irony']).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2568, 98) (2568, 2)\n",
      "(1266, 98) (1266, 2)\n"
     ]
    }
   ],
   "source": [
    "# split the data in Test and Train\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size = 0.33, random_state = 42)\n",
    "print(X_train.shape,Y_train.shape)\n",
    "print(X_test.shape,Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_7 (Embedding)      (None, 98, 20)            263820    \n",
      "_________________________________________________________________\n",
      "lstm_10 (LSTM)               (None, 98, 20)            3280      \n",
      "_________________________________________________________________\n",
      "lstm_11 (LSTM)               (None, 98, 10)            1240      \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 98, 10)            0         \n",
      "_________________________________________________________________\n",
      "flatten_6 (Flatten)          (None, 980)               0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 2)                 1962      \n",
      "=================================================================\n",
      "Total params: 270,302\n",
      "Trainable params: 270,302\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 2568 samples, validate on 1266 samples\n",
      "Epoch 1/10\n",
      " - 83s - loss: 0.6939 - categorical_accuracy: 0.5047 - val_loss: 0.6928 - val_categorical_accuracy: 0.5118\n",
      "Epoch 2/10\n",
      " - 118s - loss: 0.6709 - categorical_accuracy: 0.5927 - val_loss: 0.6453 - val_categorical_accuracy: 0.6161\n",
      "Epoch 3/10\n",
      " - 115s - loss: 0.3548 - categorical_accuracy: 0.8481 - val_loss: 0.8487 - val_categorical_accuracy: 0.6035\n",
      "Epoch 4/10\n",
      " - 96s - loss: 0.0868 - categorical_accuracy: 0.9720 - val_loss: 1.1708 - val_categorical_accuracy: 0.6011\n",
      "Epoch 5/10\n",
      " - 85s - loss: 0.0193 - categorical_accuracy: 0.9938 - val_loss: 1.7096 - val_categorical_accuracy: 0.6051\n",
      "Epoch 6/10\n",
      " - 86s - loss: 0.0057 - categorical_accuracy: 0.9988 - val_loss: 2.0872 - val_categorical_accuracy: 0.6035\n",
      "Epoch 7/10\n",
      " - 96s - loss: 0.0071 - categorical_accuracy: 0.9977 - val_loss: 1.9281 - val_categorical_accuracy: 0.5979\n",
      "Epoch 8/10\n",
      " - 96s - loss: 0.0059 - categorical_accuracy: 0.9981 - val_loss: 2.2143 - val_categorical_accuracy: 0.5877\n",
      "Epoch 9/10\n",
      " - 106s - loss: 9.0262e-04 - categorical_accuracy: 1.0000 - val_loss: 2.5299 - val_categorical_accuracy: 0.5885\n",
      "Epoch 10/10\n",
      " - 109s - loss: 2.4350e-04 - categorical_accuracy: 1.0000 - val_loss: 2.7106 - val_categorical_accuracy: 0.5845\n"
     ]
    }
   ],
   "source": [
    "# assign the hidden layer size for first lstm\n",
    "embed_dim = 20\n",
    "# assign hidden layer size for second lstm\n",
    "lstm_out = 10\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_fatures, embed_dim,input_length = X.shape[1]))\n",
    "# model.add(Dense(embed_dim, input_dim=X.shape[1], activation='relu', activity_regularizer=l1(0.0001)))\n",
    "model.add(LSTM(embed_dim,activation='relu', return_sequences=True))\n",
    "model.add(LSTM(lstm_out, return_sequences=True))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(2,activation='softmax'))\n",
    "model.compile(loss = 'categorical_crossentropy', optimizer='adam',metrics = ['categorical_accuracy'])\n",
    "print(model.summary())\n",
    "\n",
    "batch_size = 20\n",
    "model.fit(X_train, Y_train, epochs = 10, validation_data = (X_test, Y_test),\n",
    "          batch_size=batch_size, verbose = 2)\n",
    "\n",
    "# Save the model\n",
    "model.save(\"final_model.hdf5_updt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 0.59\n"
     ]
    }
   ],
   "source": [
    "model_updt = load_model(\"final_model.hdf5_updt\")\n",
    "\n",
    "validation_size = 1000\n",
    "\n",
    "X_validate = X_test[-validation_size:]\n",
    "Y_validate = Y_test[-validation_size:]\n",
    "score,acc = model_updt.evaluate(X_validate, Y_validate, verbose = 2, batch_size = batch_size)\n",
    "print(\"acc: %.2f\" % (acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have tested with 1200 test cases and the accuracy has increased slightly  from around 58% to around 62%\n",
    "But at the same time we have seen the Model has overfit, to reduce the overfitting we can have the following approach:\n",
    "\n",
    "* We need more data to train the classifier\n",
    "* Initialise with more more efficient word embedding and create more efficient feature vector\n",
    "* Implement Weight Regularization\n",
    "\n",
    "\n",
    "#### Bibliography\n",
    "\n",
    "1. Liip. (2019). Sentiment detection with Keras, word embeddings and LSTM deep learning networks · Blog · Liip. [online] Available at: https://www.liip.ch/en/blog/sentiment-detection-with-keras-word-embeddings-and-lstm-deep-learning-networks [Accessed 1 Mar. 2019]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Advanced Topics in Natural Language Processing - Assignment 1.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
